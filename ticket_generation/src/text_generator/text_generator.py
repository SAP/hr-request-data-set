"""
This module leverages a language model (e.g. GPT-J) for generating synthetic HR tickets.
The text generation is guided by combining fixed prompts with the synthetic attributes
generated by the employee_generator.employeeGenerator class.
"""
from __future__ import annotations

import json
import os
import random
import re
from abc import ABC, abstractmethod
from datetime import datetime
from string import Template
from typing import Any

import pandas as pd
import torch
from tqdm import tqdm
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, LogitsProcessorList

from util import entityType


class TicketTextGenerator(ABC):
    """
    This class is responsible for the text generation of synthetic tickets.

    Attributes:
        model: language model for the text generation.
        tokenizer: tokenizer to be used.
        spacy_model: spacy model used for post-processing.
        data_path: path where data are located ( There should be a folder called templates in this folder)
        create_only_first_part: boolean that if set equal to True says to the generator to create
                                only the initial prompt of tickets
        file_name: file name of the templates json file
    """

    def __init__(
        self,
        model: GPT2LMHeadModel,
        tokenizer: GPT2TokenizerFast,
        spacy_model,
        logits_processor: LogitsProcessorList,
        data_path: str,
        create_only_first_part: bool,
        file_name: str,
        min_length: int,
        word_limit: int,
        category: str,
        sub_category: str,
        use_gpu: bool,
        device: str,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.spacy_model = spacy_model
        self.data_path = data_path
        self.create_only_first_part = create_only_first_part
        self.file_name = file_name
        self.min_length = min_length
        self.word_limit = word_limit
        self.category = category
        self.sub_category = sub_category
        self.use_gpu = use_gpu
        self.device = device
        self.logits_processor = logits_processor

    def _preprocess_template(
        self,
        template: str,
        templates_initial_info: list[str],
        templates_additional_info: list[str],
        templates_subject: str,
        create_only_first_part: bool,
    ) -> str:
        """
        Preprocessing steps:
        - Join template arrays ( it's an array in the JSON for readability )

        Args:
            template (str): template's main text ( Ex. 'Dear Sir/Madam, I am writing to...')
            templates_initial_info (list[str]): list of lines at the start of a template
                                                ( From:..., To:..., Company:..., ...)
            templates_additional_info (list[str]): list of lines of additional info of a template
                                                   ( Ex: " Days of absence: {days_of_absence}" )
            templates_subject (str): template's subject
            create_only_first_part (bool): If set to True, remove ticket text that is not the initial prompt
        Returns:
            str: preprocessed template
        """
        _template: str
        _template = "".join([*templates_initial_info, *templates_additional_info, templates_subject])

        if not create_only_first_part:
            _template += template

        return _template

    def _postprocess_ticket_text(self, ticket: str) -> str:
        """
        Operations done on the ticket after enhancement with gpt2:
        - Removal of double/triple dots

        Args:
            ticket (str):

        Returns:
            str: processed ticket
        """

        consecutive_dots = re.compile(r"\.{2,}")
        _ticket: str = consecutive_dots.sub(".", ticket)

        return _ticket

    def _get_variables_for_ner(self) -> list[str]:
        """
        Get from templates.json file the variables' names that you want to include for named entity recognition

        Returns:
            list[str]: list of variables' names
        """

        with open(f"{self.data_path}/templates/{self.file_name}") as f:
            # Get all tickets for all categories
            templates: dict = json.load(f)

        # Get tickets and other info for the ticket's category needed
        templates_specific: dict[str, Any] = templates["tickets"][self.category][self.sub_category]

        # Get variables'names for NER
        templates_variables: list[str] = templates_specific["variables"]

        return templates_variables

    def _get_templates(self, create_only_first_part: bool) -> list[Template]:
        """
        Get the templates of synthetic tickets from a json file saved in self.data_path

        Returns:
            list[Template]: templates formatted as strings
        """
        with open(f"{self.data_path}/templates/{self.file_name}") as f:
            # Get all tickets for all categories
            templates: dict = json.load(f)

        # Get tickets and other info for the ticket's category needed
        templates_specific: dict[str, Any] = templates["tickets"][self.category][self.sub_category]

        templates_initial_info: list[str] = templates["initial_info"]
        templates_additional_info: list[str] = templates_specific["additional_info"]
        templates_subject: str = templates_specific["subject"]
        templates_text: list[str] = templates_specific["templates"]

        # Preproces templates ( remove newlines characters )
        templates_text = list(
            map(
                lambda t: self._preprocess_template(
                    t, templates_initial_info, templates_additional_info, templates_subject, create_only_first_part
                ),
                templates_text,
            )
        )

        templates_to_return: list[Template] = list(map(lambda t: Template(t), templates_text))

        return templates_to_return

    def generate_line(self, previous_txt: str, word_limit: int) -> tuple[str, str]:
        """
        Uses previous text and a given prompt to generate a new piece of text.

        Args:
            previous_txt: previous text.
            word_limit: maximum number of words to be generated.
        Returns:
            new_txt: model output which includes previous text.
            added_txt: newly generated text.
        """

        input_txt: str = f"{previous_txt}"

        encoding_dicts = self.tokenizer(input_txt, truncation=True, add_special_tokens=True)
        input_ids = torch.tensor([encoding_dicts["input_ids"]])
        attention_mask = torch.tensor([encoding_dicts["attention_mask"]])

        if self.use_gpu and next(self.model.parameters()).is_cuda:
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)

        added_txt: str = ""
        # I force GPT to create some text, if GPT does not add any text I re-run GPT
        while added_txt == "":
            new_txt = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                do_sample=self.model.do_sample_,
                min_length=self.min_length,
                max_length=len(input_ids[0]) + word_limit,
                top_p=self.model.top_p_,
                top_k=self.model.top_k_,
                repetition_penalty=self.model.repetition_penalty_,
                temperature_=self.model.temperature_,
                length_penalty=self.model.length_penalty_,
                no_repeat_ngram_size=self.model.no_repeat_ngram_size_,
                bad_words_ids=self.model.bad_words_ids_,
                force_words_ids=self.model.force_words_ids_,
                num_beams=self.model.num_beams_,
                pad_token_id=self.tokenizer.eos_token_id,
                logits_processor=self.logits_processor,
            )
            new_txt_decoded: str = self.tokenizer.decode(new_txt[0], skip_special_tokens=True)

            added_txt: str = new_txt_decoded.replace(previous_txt, "")
            if added_txt != "":
                added_txt = self._polish_generated_text(new_txt_decoded.replace(previous_txt, ""))

        new_txt = f"{previous_txt}{added_txt}"
        return new_txt, added_txt

    def _polish_generated_text(self, txt: str) -> str:
        """
        Parses a piece of text using a spacy language model and retrieves the first sentence only.
        This is additionally processed to remove any superflous whitespace.

        Args:
            txt: input text.
        Returns:
            the first sentence in the input text.
        """
        sentence_list = [sentence for sentence in self.spacy_model(txt).sents]
        sentence_list_texts: list[str] = [sentence.text for sentence in sentence_list]
        line: str = "".join(sentence_list_texts[:-1]) if len(sentence_list_texts) > 1 else sentence_list_texts[0]
        line = re.sub(r"\s+", " ", line).strip()
        return line

    def enhance_ticket(self, synthetic_ticket: str) -> str:
        splitted_synthetic_ticket = synthetic_ticket.split("<generate>")

        ticket_text: str = ""
        for i, section in enumerate(splitted_synthetic_ticket):
            ticket_text += section

            # After the last section of the ticket, we don't want to generate new text
            if i == len(splitted_synthetic_ticket) - 1:
                break

            new_txt, _ = self.generate_line(ticket_text, self.word_limit)
            ticket_text: str = new_txt

        ticket_text = self._postprocess_ticket_text(ticket_text)

        return ticket_text

    def process_variables(self, variables: list[str]) -> dict[str, str]:
        """
        Method to make variables that have different name the same
        Example: "airport_form", "airport_to" --> both become "airport"
        In case there is no variables' names to be changed, the method is not reimplemented in
        the inherited classes.

        Args:
            variables (list[str]): list of variables

        Returns:
            dict[str, str]: dictionary with key equal to old name and value equal to new name
                            most of the variables will have the same old and new name
        """
        # All variables has same key and value by default (they do not change name)
        variables_processed = {v: v for v in variables}

        return variables_processed

    def get_entities_exact_match(
        self, ticket: str, employee: dict[str, str], variables: dict[str, str]
    ) -> list[entityType]:
        """
        From a ticket text(str) and an employee(dictionary with all features of employee) this function
        find all entities(features) of employee in the text
        The entities are dependanr on the type of ticket. The features that this method looks for are defined in
        the templates.json file and passed as 'variables'
        ( EX: "I want to request 2 days off" --> "2 days" : duration_of_absence --> (18,24,duration_of_absence) )

        Args:
            ticket (str): ticket text string
            employee (dict[str, str]): employee dictionary
            variables (dict[str,str]): list of variables' names of entity that you want to track for NER

        Returns:
            list[entityType]: list of found entities(features) of employee inside the text
                              format: [( Start_character_index, End_Character_index, Entity_Name), ...]
        """
        entities_found_in_ticket_text: list[entityType] = []

        # Loop over all features of an employee
        for entity in employee:

            # If variable not present in variables I want to mark I skip it
            if entity not in variables.keys():
                continue

            # Convert old name of variable to new name
            entity_name: str = variables[entity]

            # Find all match of the feature in the text
            for match in re.finditer(pattern=str(employee[entity]), string=ticket):
                _entity_found = (match.start(), match.end(), entity_name)
                entities_found_in_ticket_text.append(_entity_found)

        return entities_found_in_ticket_text

    @abstractmethod
    def get_other_entities(self, ticket: str, employee: dict[str, str]) -> list[entityType]:
        """
        From a ticket text(str) this function find all entities(features) of employee in the text that
        are not exact matches, but that have similar meaning ( Ex. maybe a date is saved in an employee as
        01/10/1998 but in the text it appears as 1st October 1998, so we would like to mark it also as a date)

        Args:
            ticket (str): ticket text string

        Returns:
            list[entityType]: list of found entities(features) of employee inside the text
                              format: [( Start_character_index, End_Character_index, Entity_Name), ...]
        """

    def filter_redundant_entities(self, entities: list[entityType]):
        # Sort entities by order of appearence
        entities_sorted = sorted(entities, key=lambda e: (e[0], -e[1]))

        # Filter entities if some characters share more than one entity
        #
        # From each entity checks if last character index is greater than first character index of
        # next entity. If it is, it means they overlap. So I delete the second entity, which is the
        # one that occurs after, or that occurs at the same character but it is shorter ( arbitrary
        # choice).
        # This works because I ordered the array before
        #
        # Example
        # 1st iteration: [[3, 7], [4, 8], [5, 9], [8, 11], [8, 10], [11, 14]], i = 0
        # 2nd iteration: [[3, 7], [5, 9], [8, 11], [8, 10], [11, 14]], i = 0
        # 3rd iteration: [[3, 7], [8, 11], [8, 10], [11, 14]], i = 0
        # 4th iteration: [[3, 7], [8, 11], [8, 10], [11, 14]], i = 1
        # 5th iteration: [[3, 7], [8, 11], [11, 14]], i = 1
        # 6th iteration: [[3, 7], [8, 11], [11, 14]], i = 2 --> EXIT WHILE LOOP
        i = 0
        while i + 1 < len(entities_sorted):
            if entities_sorted[i][1] >= entities_sorted[i + 1][0]:
                entities_sorted.pop(i + 1)
            else:
                i += 1
        return entities_sorted

    @staticmethod
    def save_tickets_and_entities_to_file(
        tickets: list[str],
        entities: list[list[entityType]],
        ticket_type: str,
        output_path: str = "ticket_generation/output",
    ) -> None:
        """
        Save tickets to a json file on the output folder, for each run a different file is
        created, with year_month_day_hour_minutes at the end of the filename

        Args:
            tickets (list[str]): tickets texts generated
            entities (list[list[entityType]]): list of entities found for each ticket ( therefore list of lists )
        """
        _time: str = datetime.today().strftime("%Y_%m_%d_%H_%M")

        file_path: str = f"{output_path}/{ticket_type}/"

        output_path_exist: bool = os.path.exists(output_path)
        file_path_exist: bool = os.path.exists(file_path)

        # If output folder does not exist I create it
        if not output_path_exist:
            os.makedirs(output_path)

        # If output folder specific to ticket type does not exist I create it
        if not file_path_exist:
            os.makedirs(file_path)

        assert len(tickets) == len(entities), "Error creation entities"

        tickets_entities: list[dict[str, Any]] = []
        for ticket, entities_ticket in zip(tickets, entities):
            tickets_entities.append({"ticket": ticket, "entities": entities_ticket})

        with open(f"{file_path}/tickets_{_time}.json", "w") as f:
            json.dump(tickets_entities, f)

    @abstractmethod
    def generate_employee(self, employee: pd.Series, **kwargs) -> dict[str, str]:
        """
        Method that receives as input an employee and adds additional features to
        the employee, like category, sub_category and features correlated with
        the ticket type.

        Args:
            employee (pd.Series):

        Returns:
            dict[str, str]: new employee
        """

    def get_kwargs(self) -> dict[str, Any]:
        """
        Method used in case generate_employee method needs additional data beyond
        the features of the employee. Reimplemented in inherited classes
        By default, it returns an empty dict

        Returns:
            dict[str, Any]: dictionary with key equal to "name_data_needed" and value the data itself
        """
        return {}

    def generate_tickets(self, employees_df: pd.DataFrame) -> tuple[list[str], list[list[entityType]]]:
        """
        Generate tickets substituting the variables and generating text with GPT where <generate>
        are located

        Args:
            employees_df (pd.DataFrame): df of employees that must have a first_name, a last_name,
            a Reason_for_absence and Absenteeism_time_in_days

        Returns:
            List[str]: generated tickets' texts
        """

        kwargs: dict[str, Any] = self.get_kwargs()

        synthetic_tickets_enhanced: list[str] = list()
        entities: list[list[entityType]] = list()

        templates: list[Template] = self._get_templates(create_only_first_part=self.create_only_first_part)
        variables_for_ner: list[str] = self._get_variables_for_ner()

        variables_processed: dict[str, str] = self.process_variables(variables_for_ner)

        for _, employee in tqdm(employees_df.iterrows()):

            # pick a random template
            template: Template = random.choice(templates)

            employee_new: dict[str, str] = self.generate_employee(employee, **kwargs)

            template_filled: str = template.safe_substitute(**employee_new)

            # If variable `create_only_first_part` is set to True, than I don't launch
            # the GPT model to create the ticket text, but I append only the first
            # part of the ticket, without also launching the entity NER
            if not self.create_only_first_part:
                # Substitute <generate> in templates with text generated with GPT2
                synthetic_ticket_enhanced: str = self.enhance_ticket(template_filled)
                synthetic_tickets_enhanced.append(synthetic_ticket_enhanced)
            else:
                synthetic_tickets_enhanced.append(template_filled)
                entities.append([])

                continue

            # Get entities from text for each generated ticket( positions + entity text)
            #
            # Ex. for a ticket
            # [( 14, 20, "prev_salary"), ...]
            # which correspond to [( Start_character_index, End_Character_index, Entity_Name)]
            entities_ticket: list[entityType] = self.get_entities_exact_match(
                synthetic_ticket_enhanced, employee_new, variables_processed
            )

            # Get other entities ( Detailed explanation inside the method )
            other_entities: list[entityType] = self.get_other_entities(synthetic_ticket_enhanced, employee_new)

            entities_ticket.extend(other_entities)

            # Filter entities ( Detailed explanation inside the method )
            entities_ticket_filtered = self.filter_redundant_entities(entities_ticket)

            entities.append(entities_ticket_filtered)

        return synthetic_tickets_enhanced, entities
